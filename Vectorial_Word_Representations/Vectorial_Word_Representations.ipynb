{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorial Word Representations\n",
    "\n",
    "## Background\n",
    "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for language-oriented machine learning architectures. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain a de facto point of reference as well as a good starting point. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Much of the code and data pre-processing has already been done for you. Additionally, notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment.\n",
    "\n",
    "For conceptual questions, give short but descriptive answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics. \n",
    "\n",
    "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a simplistic data structure that counts the amount of times each word has appeared within the context of each other word. The definition of a context varies between implementations; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word. \n",
    "\n",
    "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
    "\n",
    "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
    "\n",
    "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
    "\n",
    "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
    "\n",
    "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
    "\n",
    "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">he</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we have prepared a co-occurrence matrix over a minimally processed version of the Harry Potter books. The pickle file contains three items:\n",
    "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
    "1. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to ints\n",
    "2. `X`: a torch LongTensor $\\mathbf{X}$ of size $N \\times N$, where $\\mathbf{X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
    "\n",
    "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import Dict, Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5359\n"
     ]
    }
   ],
   "source": [
    "with open('output.p', 'rb') as f:\n",
    "    vocab, contexts, X = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the summed context of the word 'portrait'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harry', 103),\n",
       " ('said', 97),\n",
       " ('hole', 84),\n",
       " ('ron', 49),\n",
       " ('hermione', 46),\n",
       " ('room', 41),\n",
       " ('t', 40),\n",
       " ('fat', 39),\n",
       " ('lady', 39),\n",
       " ('common', 30),\n",
       " ('dumbledore', 26),\n",
       " ('phineas', 24),\n",
       " ('climbed', 22),\n",
       " ('just', 19),\n",
       " ('swung', 17),\n",
       " ('sirius', 17),\n",
       " ('professor', 17),\n",
       " ('time', 16),\n",
       " ('voice', 15),\n",
       " ('open', 15),\n",
       " ('got', 15),\n",
       " ('nigellus', 15),\n",
       " ('reached', 14),\n",
       " ('like', 14),\n",
       " ('came', 14),\n",
       " ('little', 14),\n",
       " ('gryffindor', 13),\n",
       " ('turned', 13),\n",
       " ('forward', 12),\n",
       " ('don', 12),\n",
       " ('long', 12),\n",
       " ('place', 12),\n",
       " ('wall', 11),\n",
       " ('neville', 11),\n",
       " ('black', 11),\n",
       " ('going', 11),\n",
       " ('snape', 11),\n",
       " ('hall', 11),\n",
       " ('mcgonagall', 11),\n",
       " ('corridor', 10),\n",
       " ('walked', 10),\n",
       " ('away', 10),\n",
       " ('ve', 10),\n",
       " ('way', 10),\n",
       " ('visit', 10),\n",
       " ('good', 10),\n",
       " ('did', 10),\n",
       " ('look', 10),\n",
       " ('password', 9),\n",
       " ('moment', 9),\n",
       " ('know', 9),\n",
       " ('sir', 9),\n",
       " ('opened', 9),\n",
       " ('face', 9),\n",
       " ('heard', 9),\n",
       " ('come', 8),\n",
       " ('gone', 8),\n",
       " ('asked', 8),\n",
       " ('let', 8),\n",
       " ('really', 8),\n",
       " ('entrance', 8),\n",
       " ('cadogan', 8),\n",
       " ('mother', 8),\n",
       " ('door', 8),\n",
       " ('pushed', 7),\n",
       " ('inside', 7),\n",
       " ('cloak', 7),\n",
       " ('looked', 7),\n",
       " ('picture', 7),\n",
       " ('staircase', 7),\n",
       " ('oh', 7),\n",
       " ('tower', 7),\n",
       " ('head', 7),\n",
       " ('ginny', 7),\n",
       " ('thought', 7),\n",
       " ('passed', 7),\n",
       " ('minister', 7),\n",
       " ('fudge', 7),\n",
       " ('scrambled', 6),\n",
       " ('looking', 6),\n",
       " ('bed', 6),\n",
       " ('hagrid', 6),\n",
       " ('himself', 6),\n",
       " ('point', 6),\n",
       " ('house', 6),\n",
       " ('castle', 6),\n",
       " ('tried', 6),\n",
       " ('coming', 6),\n",
       " ('need', 6),\n",
       " ('hurried', 6),\n",
       " ('stood', 6),\n",
       " ('saw', 6),\n",
       " ('people', 6),\n",
       " ('knew', 6),\n",
       " ('frame', 6),\n",
       " ('eyes', 6),\n",
       " ('dean', 6),\n",
       " ('wizard', 6),\n",
       " ('grimmauld', 6),\n",
       " ('headmaster', 6),\n",
       " ('won', 5),\n",
       " ('dress', 5),\n",
       " ('followed', 5),\n",
       " ('running', 5),\n",
       " ('seventh', 5),\n",
       " ('floor', 5),\n",
       " ('thing', 5),\n",
       " ('dormitory', 5),\n",
       " ('stairs', 5),\n",
       " ('pulled', 5),\n",
       " ('went', 5),\n",
       " ('m', 5),\n",
       " ('waiting', 5),\n",
       " ('yelled', 5),\n",
       " ('colin', 5),\n",
       " ('took', 5),\n",
       " ('left', 5),\n",
       " ('gryffindors', 5),\n",
       " ('talking', 5),\n",
       " ('doors', 5),\n",
       " ('felt', 5),\n",
       " ('office', 5),\n",
       " ('fred', 5),\n",
       " ('great', 5),\n",
       " ('set', 5),\n",
       " ('gave', 5),\n",
       " ('approached', 5),\n",
       " ('downstairs', 5),\n",
       " ('parchment', 5),\n",
       " ('dinner', 5),\n",
       " ('silver', 5),\n",
       " ('walk', 5),\n",
       " ('prime', 5),\n",
       " ('end', 4),\n",
       " ('hung', 4),\n",
       " ('woman', 4),\n",
       " ('pink', 4),\n",
       " ('percy', 4),\n",
       " ('reveal', 4),\n",
       " ('chair', 4),\n",
       " ('couldn', 4),\n",
       " ('care', 4),\n",
       " ('didn', 4),\n",
       " ('herself', 4),\n",
       " ('painting', 4),\n",
       " ('mind', 4),\n",
       " ('entered', 4),\n",
       " ('crept', 4),\n",
       " ('appeared', 4),\n",
       " ('stand', 4),\n",
       " ('short', 4),\n",
       " ('crowd', 4),\n",
       " ('spiral', 4),\n",
       " ('deserted', 4),\n",
       " ('caught', 4),\n",
       " ('began', 4),\n",
       " ('waited', 4),\n",
       " ('climbing', 4),\n",
       " ('large', 4),\n",
       " ('called', 4),\n",
       " ('boys', 4),\n",
       " ('past', 4),\n",
       " ('headed', 4),\n",
       " ('nearly', 4),\n",
       " ('years', 4),\n",
       " ('minutes', 4),\n",
       " ('closed', 4),\n",
       " ('later', 4),\n",
       " ('canvas', 4),\n",
       " ('able', 4),\n",
       " ('taken', 4),\n",
       " ('half', 4),\n",
       " ('simply', 4),\n",
       " ('ask', 4),\n",
       " ('sitting', 4),\n",
       " ('returned', 4),\n",
       " ('george', 4),\n",
       " ('night', 4),\n",
       " ('think', 4),\n",
       " ('lavender', 4),\n",
       " ('tell', 4),\n",
       " ('screaming', 4),\n",
       " ('old', 4),\n",
       " ('doing', 4),\n",
       " ('curtains', 4),\n",
       " ('figures', 4),\n",
       " ('ran', 4),\n",
       " ('odd', 4),\n",
       " ('painted', 4),\n",
       " ('bedroom', 4),\n",
       " ('desk', 4),\n",
       " ('barely', 4),\n",
       " ('silence', 4),\n",
       " ('explanation', 4),\n",
       " ('ugly', 4),\n",
       " ('man', 4),\n",
       " ('bag', 4),\n",
       " ('listen', 3),\n",
       " ('silk', 3),\n",
       " ('needed', 3),\n",
       " ('leg', 3),\n",
       " ('fireplace', 3),\n",
       " ('armchairs', 3),\n",
       " ('spoke', 3),\n",
       " ('believe', 3),\n",
       " ('wanted', 3),\n",
       " ('stop', 3),\n",
       " ('pig', 3),\n",
       " ('snout', 3),\n",
       " ('save', 3),\n",
       " ('packed', 3),\n",
       " ('use', 3),\n",
       " ('stopped', 3),\n",
       " ('managed', 3),\n",
       " ('climb', 3),\n",
       " ('invisibility', 3),\n",
       " ('sorry', 3),\n",
       " ('hurrying', 3),\n",
       " ('fight', 3),\n",
       " ('new', 3),\n",
       " ('impatiently', 3),\n",
       " ('standing', 3),\n",
       " ('leaving', 3),\n",
       " ('creevey', 3),\n",
       " ('lockhart', 3),\n",
       " ('trolls', 3),\n",
       " ('pointing', 3),\n",
       " ('wait', 3),\n",
       " ('gray', 3),\n",
       " ('immediately', 3),\n",
       " ('closing', 3),\n",
       " ('corridors', 3),\n",
       " ('hidden', 3),\n",
       " ('trouble', 3),\n",
       " ('threw', 3),\n",
       " ('right', 3),\n",
       " ('watch', 3),\n",
       " ('d', 3),\n",
       " ('rest', 3),\n",
       " ('students', 3),\n",
       " ('heads', 3),\n",
       " ('vanished', 3),\n",
       " ('crookshanks', 3),\n",
       " ('shut', 3),\n",
       " ('dormitories', 3),\n",
       " ('completely', 3),\n",
       " ('mad', 3),\n",
       " ('shall', 3),\n",
       " ('outside', 3),\n",
       " ('read', 3),\n",
       " ('tiny', 3),\n",
       " ('walls', 3),\n",
       " ('extremely', 3),\n",
       " ('met', 3),\n",
       " ('table', 3),\n",
       " ('grounds', 3),\n",
       " ('corner', 3),\n",
       " ('sight', 3),\n",
       " ('told', 3),\n",
       " ('circular', 3),\n",
       " ('hand', 3),\n",
       " ('silent', 3),\n",
       " ('noise', 3),\n",
       " ('eye', 3),\n",
       " ('admit', 3),\n",
       " ('stay', 3),\n",
       " ('clambered', 3),\n",
       " ('realized', 3),\n",
       " ('mrs', 3),\n",
       " ('taking', 3),\n",
       " ('expression', 3),\n",
       " ('movement', 3),\n",
       " ('hair', 3),\n",
       " ('slightly', 3),\n",
       " ('wearing', 3),\n",
       " ('wand', 3),\n",
       " ('trying', 3),\n",
       " ('dark', 3),\n",
       " ('marching', 3),\n",
       " ('calling', 3),\n",
       " ('introduce', 3),\n",
       " ('announced', 3),\n",
       " ('arrival', 3),\n",
       " ('mentioned', 3),\n",
       " ('romilda', 3),\n",
       " ('vane', 3),\n",
       " ('word', 3),\n",
       " ('sped', 3),\n",
       " ('witches', 3),\n",
       " ('armor', 3),\n",
       " ('steps', 3),\n",
       " ('girl', 3),\n",
       " ('ariana', 3),\n",
       " ('severus', 3),\n",
       " ('bloody', 2),\n",
       " ('control', 2),\n",
       " ('prefects', 2),\n",
       " ('glowing', 2),\n",
       " ('shadows', 2),\n",
       " ('wasn', 2),\n",
       " ('easily', 2),\n",
       " ('angry', 2),\n",
       " ('train', 2),\n",
       " ('tomorrow', 2),\n",
       " ('nighttime', 2),\n",
       " ('shoulders', 2),\n",
       " ('toppled', 2),\n",
       " ('guess', 2),\n",
       " ('stuck', 2),\n",
       " ('clock', 2),\n",
       " ('burst', 2),\n",
       " ('clearly', 2),\n",
       " ('cut', 2),\n",
       " ('arrive', 2),\n",
       " ('arms', 2),\n",
       " ('better', 2),\n",
       " ('camera', 2),\n",
       " ('longbottom', 2),\n",
       " ('held', 2),\n",
       " ('winking', 2),\n",
       " ('shoulder', 2),\n",
       " ('quidditch', 2),\n",
       " ('practice', 2),\n",
       " ('watched', 2),\n",
       " ('board', 2),\n",
       " ('justin', 2),\n",
       " ('usually', 2),\n",
       " ('snow', 2),\n",
       " ('counting', 2),\n",
       " ('distant', 2),\n",
       " ('sounds', 2),\n",
       " ('throwing', 2),\n",
       " ('teachers', 2),\n",
       " ('slid', 2),\n",
       " ('miserable', 2),\n",
       " ('crossed', 2),\n",
       " ('lot', 2),\n",
       " ('marble', 2),\n",
       " ('girls', 2),\n",
       " ('hasn', 2),\n",
       " ('glad', 2),\n",
       " ('weren', 2),\n",
       " ('second', 2),\n",
       " ('work', 2),\n",
       " ('turn', 2),\n",
       " ('feast', 2),\n",
       " ('glancing', 2),\n",
       " ('isn', 2),\n",
       " ('curiously', 2),\n",
       " ('closer', 2),\n",
       " ('torn', 2),\n",
       " ('whisper', 2),\n",
       " ('moving', 2),\n",
       " ('hiding', 2),\n",
       " ('ripped', 2),\n",
       " ('firmly', 2),\n",
       " ('cloaks', 2),\n",
       " ('oak', 2),\n",
       " ('christmas', 2),\n",
       " ('party', 2),\n",
       " ('previous', 2),\n",
       " ('headmasters', 2),\n",
       " ('sat', 2),\n",
       " ('carried', 2),\n",
       " ('staring', 2),\n",
       " ('stared', 2),\n",
       " ('delighted', 2),\n",
       " ('match', 2),\n",
       " ('getting', 2),\n",
       " ('weasley', 2),\n",
       " ('shaking', 2),\n",
       " ('breath', 2),\n",
       " ('em', 2),\n",
       " ('landing', 2),\n",
       " ('laughing', 2),\n",
       " ('bit', 2),\n",
       " ('concealed', 2),\n",
       " ('peeves', 2),\n",
       " ('ears', 2),\n",
       " ('join', 2),\n",
       " ('potter', 2),\n",
       " ('say', 2),\n",
       " ('annoyed', 2),\n",
       " ('laugh', 2),\n",
       " ('year', 2),\n",
       " ('demanded', 2),\n",
       " ('seen', 2),\n",
       " ('friend', 2),\n",
       " ('yell', 2),\n",
       " ('woke', 2),\n",
       " ('shown', 2),\n",
       " ('mood', 2),\n",
       " ('quite', 2),\n",
       " ('life', 2),\n",
       " ('rolling', 2),\n",
       " ('wake', 2),\n",
       " ('led', 2),\n",
       " ('added', 2),\n",
       " ('hear', 2),\n",
       " ('free', 2),\n",
       " ('idea', 2),\n",
       " ('scowled', 2),\n",
       " ('halt', 2),\n",
       " ('correct', 2),\n",
       " ('revealing', 2),\n",
       " ('talk', 2),\n",
       " ('kind', 2),\n",
       " ('light', 2),\n",
       " ('kept', 2),\n",
       " ('allowed', 2),\n",
       " ('sound', 2),\n",
       " ('dead', 2),\n",
       " ('cold', 2),\n",
       " ('covered', 2),\n",
       " ('hastily', 2),\n",
       " ('feet', 2),\n",
       " ('seamus', 2),\n",
       " ('panting', 2),\n",
       " ('view', 2),\n",
       " ('st', 2),\n",
       " ('mungo', 2),\n",
       " ('looks', 2),\n",
       " ('bad', 2),\n",
       " ('clever', 2),\n",
       " ('pointed', 2),\n",
       " ('slytherin', 2),\n",
       " ('longer', 2),\n",
       " ('message', 2),\n",
       " ('traveling', 2),\n",
       " ('study', 2),\n",
       " ('healer', 2),\n",
       " ('sideways', 2),\n",
       " ('queue', 2),\n",
       " ('shining', 2),\n",
       " ('middle', 2),\n",
       " ('amused', 2),\n",
       " ('apparently', 2),\n",
       " ('red', 2),\n",
       " ('forest', 2),\n",
       " ('dawn', 2),\n",
       " ('arguing', 2),\n",
       " ('broken', 2),\n",
       " ('pictures', 2),\n",
       " ('dippet', 2),\n",
       " ('owe', 2),\n",
       " ('hurtled', 2),\n",
       " ('ignoring', 2),\n",
       " ('tonight', 2),\n",
       " ('naturally', 2),\n",
       " ('uncomfortable', 2),\n",
       " ('curly', 2),\n",
       " ('digging', 2),\n",
       " ('ear', 2),\n",
       " ('wish', 2),\n",
       " ('image', 2),\n",
       " ('number', 2),\n",
       " ('boy', 2),\n",
       " ('joined', 2),\n",
       " ('okay', 2),\n",
       " ('feeling', 2),\n",
       " ('darted', 2),\n",
       " ('late', 2),\n",
       " ('make', 2),\n",
       " ('merely', 2),\n",
       " ('sort', 2),\n",
       " ('leapt', 2),\n",
       " ('aside', 2),\n",
       " ('fell', 2),\n",
       " ('cried', 2),\n",
       " ('help', 2),\n",
       " ('hit', 2),\n",
       " ('small', 2),\n",
       " ('group', 2),\n",
       " ('understood', 2),\n",
       " ('chest', 2),\n",
       " ('folded', 2),\n",
       " ('thinking', 2),\n",
       " ('hogwarts:', 2),\n",
       " ('moon', 2),\n",
       " ('spectacles', 2),\n",
       " ('died', 2),\n",
       " ('forgotten', 2),\n",
       " ('bring', 2),\n",
       " ('departure', 2),\n",
       " ('gruffly', 2),\n",
       " ('larger', 2),\n",
       " ('fang', 2),\n",
       " ('galloping', 2),\n",
       " ('alongside', 2),\n",
       " ('wizards', 2),\n",
       " ('mudblood', 2),\n",
       " ('scene', 2),\n",
       " ('tears', 2),\n",
       " ('voldemort', 2),\n",
       " ('granger', 2),\n",
       " ('sword', 2),\n",
       " ('baron', 1),\n",
       " ('round', 1),\n",
       " ('turning', 1),\n",
       " ('hunched', 1),\n",
       " ('nearest', 1),\n",
       " ('lamp', 1),\n",
       " ('hissing', 1),\n",
       " ('remember', 1),\n",
       " ('home', 1),\n",
       " ('facing', 1),\n",
       " ('cared', 1),\n",
       " ('space', 1),\n",
       " ('possible', 1),\n",
       " ('monster', 1),\n",
       " ('earth', 1),\n",
       " ('hanging', 1),\n",
       " ('flushed', 1),\n",
       " ('sweaty', 1),\n",
       " ('faces', 1),\n",
       " ('panted', 1),\n",
       " ('collapsed', 1),\n",
       " ('trembling', 1),\n",
       " ('saving', 1),\n",
       " ('hadn', 1),\n",
       " ('locked', 1),\n",
       " ('reminded', 1),\n",
       " ('noisy', 1),\n",
       " ('quickly', 1),\n",
       " ('play', 1),\n",
       " ('legs', 1),\n",
       " ('recognized', 1),\n",
       " ('locker', 1),\n",
       " ('curse', 1),\n",
       " ('midnight', 1),\n",
       " ('tail', 1),\n",
       " ('wailed', 1),\n",
       " ('desperate', 1),\n",
       " ('exploded', 1),\n",
       " ('idiot', 1),\n",
       " ('words', 1),\n",
       " ('sudden', 1),\n",
       " ('storm', 1),\n",
       " ('clapping', 1),\n",
       " ('lopsided', 1),\n",
       " ('tables', 1),\n",
       " ('squashy', 1),\n",
       " ('pull', 1),\n",
       " ('brilliant', 1),\n",
       " ('smirking', 1),\n",
       " ('mr', 1),\n",
       " ('beaming', 1),\n",
       " ('double', 1),\n",
       " ('sign', 1),\n",
       " ('fumbled', 1),\n",
       " ('bell', 1),\n",
       " ('rang', 1),\n",
       " ('picked', 1),\n",
       " ('copy', 1),\n",
       " ('gilderoy', 1),\n",
       " ('order', 1),\n",
       " ('merlin', 1),\n",
       " ('class', 1),\n",
       " ('member', 1),\n",
       " ('nimbus', 1),\n",
       " ('thousand', 1),\n",
       " ('clatter', 1),\n",
       " ('dashing', 1),\n",
       " ('swinging', 1),\n",
       " ('hurry', 1),\n",
       " ('wow', 1),\n",
       " ('game', 1),\n",
       " ('knight', 1),\n",
       " ('horse', 1),\n",
       " ('dragged', 1),\n",
       " ('important', 1),\n",
       " ('wondering', 1),\n",
       " ('darker', 1),\n",
       " ('swirling', 1),\n",
       " ('attacks', 1),\n",
       " ('urge', 1),\n",
       " ('thinks', 1),\n",
       " ('somewhat', 1),\n",
       " ('awkwardly', 1),\n",
       " ('ghost', 1),\n",
       " ('ravenclaw', 1),\n",
       " ('seizing', 1),\n",
       " ('difficult', 1),\n",
       " ('journey', 1),\n",
       " ('dodging', 1),\n",
       " ('weasleys', 1),\n",
       " ('darkness', 1),\n",
       " ('falling', 1),\n",
       " ('activity', 1),\n",
       " ('tired', 1),\n",
       " ('sadly', 1),\n",
       " ('remembering', 1),\n",
       " ('divided', 1),\n",
       " ('separate', 1),\n",
       " ('dementors', 1),\n",
       " ('things', 1),\n",
       " ('meet', 1),\n",
       " ('anybody', 1),\n",
       " ('entirely', 1),\n",
       " ('sure', 1),\n",
       " ('october', 1),\n",
       " ('halloween', 1),\n",
       " ('excellent', 1),\n",
       " ('zonko', 1),\n",
       " ('jerking', 1),\n",
       " ('chattering', 1),\n",
       " ('older', 1),\n",
       " ('forehead', 1),\n",
       " ('library', 1),\n",
       " ('choice', 1),\n",
       " ('waking', 1),\n",
       " ('grumpily', 1),\n",
       " ('checked', 1),\n",
       " ('starting', 1),\n",
       " ('discussing', 1),\n",
       " ('dropped', 1),\n",
       " ('nervously', 1),\n",
       " ('usual', 1),\n",
       " ('path', 1),\n",
       " ('ended', 1),\n",
       " ('jammed', 1),\n",
       " ('peered', 1),\n",
       " ('bustling', 1),\n",
       " ('importantly', 1),\n",
       " ('arrived', 1),\n",
       " ('sweeping', 1),\n",
       " ('squeezed', 1),\n",
       " ('moved', 1),\n",
       " ('grabbed', 1),\n",
       " ('arm', 1),\n",
       " ('slashed', 1),\n",
       " ('littered', 1),\n",
       " ('map', 1),\n",
       " ('replaced', 1),\n",
       " ('happy', 1),\n",
       " ('spent', 1),\n",
       " ('sneaking', 1),\n",
       " ('breakfast', 1),\n",
       " ('yellow', 1),\n",
       " ('men', 1),\n",
       " ('enjoying', 1),\n",
       " ('couple', 1),\n",
       " ('handle', 1),\n",
       " ('shiny', 1),\n",
       " ('pointless', 1),\n",
       " ('angle', 1),\n",
       " ('accompanied', 1),\n",
       " ('certain', 1),\n",
       " ('informed', 1),\n",
       " ('heel', 1),\n",
       " ('firebolt', 1),\n",
       " ('tin', 1),\n",
       " ('high', 1),\n",
       " ('finish', 1),\n",
       " ('clutched', 1),\n",
       " ('startled', 1),\n",
       " ('eat', 1),\n",
       " ('nightmare', 1),\n",
       " ('telling', 1),\n",
       " ('slammed', 1),\n",
       " ('furiously', 1),\n",
       " ('knife', 1),\n",
       " ('ridiculous', 1),\n",
       " ('possibly', 1),\n",
       " ('gotten', 1),\n",
       " ('finger', 1),\n",
       " ('glaring', 1),\n",
       " ('suspiciously', 1),\n",
       " ('listened', 1),\n",
       " ('piece', 1),\n",
       " ('paper', 1),\n",
       " ('stunned', 1),\n",
       " ('white', 1),\n",
       " ('person', 1),\n",
       " ('mouse', 1),\n",
       " ('holes', 1),\n",
       " ('fired', 1),\n",
       " ('lonely', 1),\n",
       " ('woken', 1),\n",
       " ('thoughtfully', 1),\n",
       " ('kill', 1),\n",
       " ('total', 1),\n",
       " ('furious', 1),\n",
       " ('security', 1),\n",
       " ('fast', 1),\n",
       " ('asleep', 1),\n",
       " ('resting', 1),\n",
       " ('joking', 1),\n",
       " ('heading', 1),\n",
       " ('freedom', 1),\n",
       " ('sentence', 1),\n",
       " ('strode', 1),\n",
       " ('banging', 1),\n",
       " ('prefect', 1),\n",
       " ('crackling', 1),\n",
       " ('carrying', 1),\n",
       " ('fine', 1),\n",
       " ('worry', 1),\n",
       " ('feels', 1),\n",
       " ('normal', 1),\n",
       " ('briefly', 1),\n",
       " ('blast', 1),\n",
       " ('knocked', 1),\n",
       " ('backward', 1),\n",
       " ('wrenched', 1),\n",
       " ('dozen', 1),\n",
       " ('allow', 1),\n",
       " ('cornered', 1),\n",
       " ('brothers', 1),\n",
       " ('frantically', 1),\n",
       " ('resolutely', 1),\n",
       " ('hello', 1),\n",
       " ('instead', 1),\n",
       " ('far', 1),\n",
       " ('badges', 1),\n",
       " ('minute', 1),\n",
       " ('keeping', 1),\n",
       " ('dare', 1),\n",
       " ('slow', 1),\n",
       " ('gasped', 1),\n",
       " ('muttered', 1),\n",
       " ('sleepily', 1),\n",
       " ('opening', 1),\n",
       " ('fourth', 1),\n",
       " ('bowed', 1),\n",
       " ('parvati', 1),\n",
       " ('action', 1),\n",
       " ('wishing', 1),\n",
       " ('winked', 1),\n",
       " ('o', 1),\n",
       " ('fool', 1),\n",
       " ('cho', 1),\n",
       " ('lights', 1),\n",
       " ('irritated', 1),\n",
       " ('dragons', 1),\n",
       " ('bye', 1),\n",
       " ('straight', 1),\n",
       " ('tortured', 1),\n",
       " ('size', 1),\n",
       " ('unpleasant', 1),\n",
       " ('month', 1),\n",
       " ('sticking', 1),\n",
       " ('charm', 1),\n",
       " ('quick', 1),\n",
       " ('bewildered', 1),\n",
       " ('earsplitting', 1),\n",
       " ('lupin', 1),\n",
       " ('calm', 1),\n",
       " ('kitchen', 1),\n",
       " ('seat', 1),\n",
       " ('obviously', 1),\n",
       " ('walking', 1),\n",
       " ('clattering', 1),\n",
       " ('chain', 1),\n",
       " ('deep', 1),\n",
       " ('orders', 1),\n",
       " ('foul', 1),\n",
       " ('hopefully', 1),\n",
       " ('parents', 1),\n",
       " ('sighed', 1),\n",
       " ('wouldn', 1),\n",
       " ('occasionally', 1),\n",
       " ('useful', 1),\n",
       " ('stuffed', 1),\n",
       " ('cage', 1),\n",
       " ('dragging', 1),\n",
       " ('trunk', 1),\n",
       " ('howling', 1),\n",
       " ('rage', 1),\n",
       " ('bothering', 1),\n",
       " ('close', 1),\n",
       " ('bound', 1),\n",
       " ('events', 1),\n",
       " ('graveyard', 1),\n",
       " ('er', 1),\n",
       " ('glumly', 1),\n",
       " ('positively', 1),\n",
       " ('alarmed', 1),\n",
       " ('cabin', 1),\n",
       " ('chairs', 1),\n",
       " ('sense', 1),\n",
       " ('stuff', 1),\n",
       " ('working', 1),\n",
       " ('carefully', 1),\n",
       " ('owlery', 1),\n",
       " ('headless', 1),\n",
       " ('nick', 1),\n",
       " ('drifting', 1),\n",
       " ('coolly', 1),\n",
       " ('hour', 1),\n",
       " ('lousy', 1),\n",
       " ('disheveled', 1),\n",
       " ('realize', 1),\n",
       " ('happen', 1),\n",
       " ('fair', 1),\n",
       " ('giggling', 1),\n",
       " ('madly', 1),\n",
       " ('fashioned', 1),\n",
       " ('senses', 1),\n",
       " ('early', 1),\n",
       " ('ravenclaws', 1),\n",
       " ('west', 1),\n",
       " ('finally', 1),\n",
       " ('creaking', 1),\n",
       " ('pale', 1),\n",
       " ('tracks', 1),\n",
       " ('elf', 1),\n",
       " ('hats', 1),\n",
       " ('defensively', 1),\n",
       " ('clicked', 1),\n",
       " ('tongue', 1),\n",
       " ('grown', 1),\n",
       " ('crouch', 1),\n",
       " ('prevent', 1),\n",
       " ('panic', 1),\n",
       " ('strange', 1),\n",
       " ('instrument', 1),\n",
       " ('shout', 1),\n",
       " ('reappeared', 1),\n",
       " ('news', 1),\n",
       " ('doesn', 1),\n",
       " ('blood', 1),\n",
       " ('coughing', 1),\n",
       " ('armchair', 1),\n",
       " ('thank', 1),\n",
       " ('minerva', 1),\n",
       " ('blue', 1),\n",
       " ('quivered', 1),\n",
       " ('marched', 1),\n",
       " ('beard', 1),\n",
       " ('colors', 1),\n",
       " ('jerk', 1),\n",
       " ('wide', 1),\n",
       " ('giving', 1),\n",
       " ('fake', 1),\n",
       " ('eyeing', 1),\n",
       " ('apprehensively', 1),\n",
       " ('destroyed', 1),\n",
       " ('family', 1),\n",
       " ('knows', 1),\n",
       " ('destroy', 1),\n",
       " ('before:', 1),\n",
       " ('issuing', 1),\n",
       " ('bored', 1),\n",
       " ('disappeared', 1),\n",
       " ('antidotes', 1),\n",
       " ('anti', 1),\n",
       " ('unless', 1),\n",
       " ('qualified', 1),\n",
       " ('witch', 1),\n",
       " ('young', 1),\n",
       " ('performing', 1),\n",
       " ('spot', 1),\n",
       " ('crystal', 1),\n",
       " ('bubbles', 1),\n",
       " ('ceiling', 1),\n",
       " ('vicious', 1),\n",
       " ('pace', 1),\n",
       " ('beds', 1),\n",
       " ('brain', 1),\n",
       " ('questions', 1),\n",
       " ('dreadful', 1),\n",
       " ('ideas', 1),\n",
       " ('snake', 1),\n",
       " ('animagus', 1),\n",
       " ('leaning', 1),\n",
       " ('watching', 1),\n",
       " ('squinting', 1),\n",
       " ('outline', 1),\n",
       " ('occurred', 1),\n",
       " ('probably', 1),\n",
       " ('case', 1),\n",
       " ('attacked', 1),\n",
       " ('dad', 1),\n",
       " ('visited', 1),\n",
       " ('comfort', 1),\n",
       " ('pile', 1),\n",
       " ('rat', 1),\n",
       " ('dropping', 1),\n",
       " ('passing', 1),\n",
       " ('says', 1),\n",
       " ('raising', 1),\n",
       " ('eyebrows', 1),\n",
       " ('interesting', 1),\n",
       " ('roared', 1),\n",
       " ('nosed', 1),\n",
       " ('ministry', 1),\n",
       " ('creatures', 1),\n",
       " ('examining', 1),\n",
       " ('unicorns', 1),\n",
       " ('thoroughly', 1),\n",
       " ('tempered', 1),\n",
       " ('runes', 1),\n",
       " ('exams', 1),\n",
       " ('celebration', 1),\n",
       " ('approaching', 1),\n",
       " ('occasional', 1),\n",
       " ('grunt', 1),\n",
       " ('sleeping', 1),\n",
       " ('surroundings', 1),\n",
       " ('reflected', 1),\n",
       " ('feelings', 1),\n",
       " ('pain', 1),\n",
       " ('quiet', 1),\n",
       " ('shattered', 1),\n",
       " ('pieces', 1),\n",
       " ('yells', 1),\n",
       " ('anger', 1),\n",
       " ('fright', 1),\n",
       " ('snatching', 1),\n",
       " ('hitting', 1),\n",
       " ('noticing', 1),\n",
       " ('start', 1),\n",
       " ('cutting', 1),\n",
       " ('telephone', 1),\n",
       " ('heart', 1),\n",
       " ('sank', 1),\n",
       " ('afraid', 1),\n",
       " ('hoping', 1),\n",
       " ('speak', 1),\n",
       " ('dreaming', 1),\n",
       " ('cough', 1),\n",
       " ('magic', 1),\n",
       " ('strain', 1),\n",
       " ('caused', 1),\n",
       " ('utterly', 1),\n",
       " ('terrified', 1),\n",
       " ('self', 1),\n",
       " ('bounced', 1),\n",
       " ('shaken', 1),\n",
       " ('remained', 1),\n",
       " ('encounter', 1),\n",
       " ('given', 1),\n",
       " ('private', 1),\n",
       " ('proved', 1),\n",
       " ('impossible', 1),\n",
       " ('remove', 1),\n",
       " ('art', 1),\n",
       " ('happened', 1),\n",
       " ('ago', 1),\n",
       " ('wet', 1),\n",
       " ('state', 1),\n",
       " ('considerable', 1),\n",
       " ('course', 1),\n",
       " ('busy', 1),\n",
       " ('quill', 1),\n",
       " ('catching', 1),\n",
       " ('finishing', 1),\n",
       " ('letter', 1),\n",
       " ('luck', 1),\n",
       " ('maybe', 1),\n",
       " ('scrimgeour', 1),\n",
       " ('success', 1),\n",
       " ('subsided', 1),\n",
       " ('suddenly', 1),\n",
       " ('official', 1),\n",
       " ('muggles', 1),\n",
       " ('meeting', 1),\n",
       " ('enchantment', 1),\n",
       " ('ensure', 1),\n",
       " ('owned', 1),\n",
       " ('pureblood', 1),\n",
       " ('vivid', 1),\n",
       " ('shrieking', 1),\n",
       " ('spitting', 1),\n",
       " ('flashed', 1),\n",
       " ('snapped', 1),\n",
       " ('particularly', 1),\n",
       " ('minuscule', 1),\n",
       " ('muttering', 1),\n",
       " ('promptly', 1),\n",
       " ('scarlet', 1),\n",
       " ('hope', 1),\n",
       " ('goes', 1),\n",
       " ('pair', 1),\n",
       " ('leave', 1),\n",
       " ('proceeded', 1),\n",
       " ('step', 1),\n",
       " ('statue', 1),\n",
       " ('trelawney', 1),\n",
       " ('smelled', 1),\n",
       " ('damp', 1),\n",
       " ('stalked', 1),\n",
       " ('undoubtedly', 1),\n",
       " ('pause', 1),\n",
       " ('worst', 1),\n",
       " ('darkly', 1),\n",
       " ('evening', 1),\n",
       " ('soon', 1),\n",
       " ('sinking', 1),\n",
       " ('mane', 1),\n",
       " ('bushy', 1),\n",
       " ('brown', 1),\n",
       " ('whipping', 1),\n",
       " ('unlocked', 1),\n",
       " ('classroom', 1),\n",
       " ('hi', 1),\n",
       " ('fancy', 1),\n",
       " ('thanks', 1),\n",
       " ('precisely', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts['portrait'].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the word 'ghost'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harry', 52),\n",
       " ('said', 36),\n",
       " ('nick', 22),\n",
       " ('t', 20),\n",
       " ('nearly', 20),\n",
       " ('headless', 18),\n",
       " ('know', 15),\n",
       " ('looked', 12),\n",
       " ('ron', 10),\n",
       " ('saw', 9),\n",
       " ('d', 9),\n",
       " ('ve', 9),\n",
       " ('got', 8),\n",
       " ('gryffindor', 8),\n",
       " ('years', 7),\n",
       " ('bloody', 7),\n",
       " ('baron', 7),\n",
       " ('wand', 7),\n",
       " ('cedric', 7),\n",
       " ('dumbledore', 7),\n",
       " ('just', 6),\n",
       " ('don', 6),\n",
       " ('think', 6),\n",
       " ('told', 6),\n",
       " ('slytherin', 6),\n",
       " ('staring', 6),\n",
       " ('eyes', 6),\n",
       " ('professor', 6),\n",
       " ('passed', 6),\n",
       " ('like', 6),\n",
       " ('hermione', 6),\n",
       " ('gray', 6),\n",
       " ('ruff', 5),\n",
       " ('suddenly', 5),\n",
       " ('table', 5),\n",
       " ('potter', 5),\n",
       " ('giving', 5),\n",
       " ('began', 5),\n",
       " ('tower', 5),\n",
       " ('magic', 5),\n",
       " ('binns', 5),\n",
       " ('light', 5),\n",
       " ('held', 5),\n",
       " ('head', 5),\n",
       " ('little', 5),\n",
       " ('solid', 5),\n",
       " ('really', 4),\n",
       " ('hufflepuff', 4),\n",
       " ('hat', 4),\n",
       " ('opposite', 4),\n",
       " ('seen', 4),\n",
       " ('arm', 4),\n",
       " ('need', 4),\n",
       " ('sir', 4),\n",
       " ('gaunt', 4),\n",
       " ('face', 4),\n",
       " ('silver', 4),\n",
       " ('corridor', 4),\n",
       " ('covered', 4),\n",
       " ('wide', 4),\n",
       " ('oh', 4),\n",
       " ('asked', 4),\n",
       " ('away', 4),\n",
       " ('myrtle', 4),\n",
       " ('hair', 4),\n",
       " ('thing', 4),\n",
       " ('hogwarts', 4),\n",
       " ('white', 4),\n",
       " ('stood', 4),\n",
       " ('voldemort', 4),\n",
       " ('end', 4),\n",
       " ('neville', 4),\n",
       " ('come', 4),\n",
       " ('haven', 3),\n",
       " ('peeves', 3),\n",
       " ('say', 3),\n",
       " ('doing', 3),\n",
       " ('wearing', 3),\n",
       " ('fat', 3),\n",
       " ('sat', 3),\n",
       " ('horrible', 3),\n",
       " ('plate', 3),\n",
       " ('good', 3),\n",
       " ('course', 3),\n",
       " ('seamus', 3),\n",
       " ('sitting', 3),\n",
       " ('history', 3),\n",
       " ('taught', 3),\n",
       " ('let', 3),\n",
       " ('direction', 3),\n",
       " ('moaning', 3),\n",
       " ('came', 3),\n",
       " ('help', 3),\n",
       " ('hedwig', 3),\n",
       " ('window', 3),\n",
       " ('man', 3),\n",
       " ('talking', 3),\n",
       " ('wasn', 3),\n",
       " ('surprised', 3),\n",
       " ('drifted', 3),\n",
       " ('mean', 3),\n",
       " ('girl', 3),\n",
       " ('wall', 3),\n",
       " ('floor', 3),\n",
       " ('large', 3),\n",
       " ('room', 3),\n",
       " ('attack', 3),\n",
       " ('crash', 3),\n",
       " ('gryffindors', 3),\n",
       " ('ravenclaw', 3),\n",
       " ('riddle', 3),\n",
       " ('diary', 3),\n",
       " ('diggory', 3),\n",
       " ('looking', 3),\n",
       " ('heard', 3),\n",
       " ('pain', 3),\n",
       " ('body', 3),\n",
       " ('alive', 3),\n",
       " ('gone', 3),\n",
       " ('golden', 3),\n",
       " ('emerged', 3),\n",
       " ('wizard', 3),\n",
       " ('malfoy', 3),\n",
       " ('sure', 3),\n",
       " ('given', 2),\n",
       " ('chances', 2),\n",
       " ('deserves', 2),\n",
       " ('gives', 2),\n",
       " ('bad', 2),\n",
       " ('noticed', 2),\n",
       " ('answered', 2),\n",
       " ('students', 2),\n",
       " ('clapped', 2),\n",
       " ('susan', 2),\n",
       " ('shouted', 2),\n",
       " ('weasley', 2),\n",
       " ('twins', 2),\n",
       " ('earlier', 2),\n",
       " ('patted', 2),\n",
       " ('sudden', 2),\n",
       " ('bit', 2),\n",
       " ('does', 2),\n",
       " ('sadly', 2),\n",
       " ('cut', 2),\n",
       " ('steak', 2),\n",
       " ('eaten', 2),\n",
       " ('myself', 2),\n",
       " ('brothers', 2),\n",
       " ('haired', 2),\n",
       " ('finnigan', 2),\n",
       " ('blank', 2),\n",
       " ('used', 2),\n",
       " ('boring', 2),\n",
       " ('class', 2),\n",
       " ('old', 2),\n",
       " ('m', 2),\n",
       " ('tall', 2),\n",
       " ('gliding', 2),\n",
       " ('started', 2),\n",
       " ('free', 2),\n",
       " ('soft', 2),\n",
       " ('ahead', 2),\n",
       " ('wings', 2),\n",
       " ('moment', 2),\n",
       " ('soared', 2),\n",
       " ('impatiently', 2),\n",
       " ('warning', 2),\n",
       " ('somebody', 2),\n",
       " ('forehead', 2),\n",
       " ('watched', 2),\n",
       " ('crouched', 2),\n",
       " ('walked', 2),\n",
       " ('er', 2),\n",
       " ('horses', 2),\n",
       " ('pack', 2),\n",
       " ('bearded', 2),\n",
       " ('position', 2),\n",
       " ('blowing', 2),\n",
       " ('horn', 2),\n",
       " ('leapt', 2),\n",
       " ('lifted', 2),\n",
       " ('high', 2),\n",
       " ('laughed', 2),\n",
       " ('subject', 2),\n",
       " ('teacher', 2),\n",
       " ('happened', 2),\n",
       " ('way', 2),\n",
       " ('filled', 2),\n",
       " ('safe', 2),\n",
       " ('door', 2),\n",
       " ('flew', 2),\n",
       " ('people', 2),\n",
       " ('dead', 2),\n",
       " ('seats', 2),\n",
       " ('counting', 2),\n",
       " ('school', 2),\n",
       " ('ago', 2),\n",
       " ('memory', 2),\n",
       " ('quietly', 2),\n",
       " ('rolling', 2),\n",
       " ('sleeves', 2),\n",
       " ('turn', 2),\n",
       " ('black', 2),\n",
       " ('quickly', 2),\n",
       " ('seeing', 2),\n",
       " ('things', 2),\n",
       " ('wands', 2),\n",
       " ('shadow', 2),\n",
       " ('skull', 2),\n",
       " ('mr', 2),\n",
       " ('vanished', 2),\n",
       " ('rest', 2),\n",
       " ('particularly', 2),\n",
       " ('chance', 2),\n",
       " ('opinion', 2),\n",
       " ('foot', 2),\n",
       " ('running', 2),\n",
       " ('clutching', 2),\n",
       " ('fear', 2),\n",
       " ('going', 2),\n",
       " ('water', 2),\n",
       " ('shock', 2),\n",
       " ('ripped', 2),\n",
       " ('spirit', 2),\n",
       " ('hand', 2),\n",
       " ('tip', 2),\n",
       " ('tightly', 2),\n",
       " ('thread', 2),\n",
       " ('remained', 2),\n",
       " ('itself', 2),\n",
       " ('correct', 2),\n",
       " ('spoke', 2),\n",
       " ('shaking', 2),\n",
       " ('past', 2),\n",
       " ('feels', 2),\n",
       " ('voice', 2),\n",
       " ('hey', 2),\n",
       " ('cloak', 2),\n",
       " ('floating', 2),\n",
       " ('board', 2),\n",
       " ('long', 2),\n",
       " ('tried', 2),\n",
       " ('tell', 2),\n",
       " ('difference', 2),\n",
       " ('lying', 2),\n",
       " ('tomb', 2),\n",
       " ('dunno', 2),\n",
       " ('wouldn', 2),\n",
       " ('felt', 2),\n",
       " ('length', 2),\n",
       " ('times', 2),\n",
       " ('spoken', 2),\n",
       " ('lady', 2),\n",
       " ('nodded', 2),\n",
       " ('did', 2),\n",
       " ('speak', 2),\n",
       " ('fred', 2),\n",
       " ('new', 1),\n",
       " ('right', 1),\n",
       " ('cheered', 1),\n",
       " ('hannah', 1),\n",
       " ('went', 1),\n",
       " ('sit', 1),\n",
       " ('waving', 1),\n",
       " ('merrily', 1),\n",
       " ('bones', 1),\n",
       " ('yelled', 1),\n",
       " ('feeling', 1),\n",
       " ('plunged', 1),\n",
       " ('eat', 1),\n",
       " ('delicious', 1),\n",
       " ('look', 1),\n",
       " ('watching', 1),\n",
       " ('miss', 1),\n",
       " ('introduced', 1),\n",
       " ('service', 1),\n",
       " ('prefer', 1),\n",
       " ('stiffly', 1),\n",
       " ('interrupted', 1),\n",
       " ('winning', 1),\n",
       " ('slytherins', 1),\n",
       " ('cup', 1),\n",
       " ('row', 1),\n",
       " ('robes', 1),\n",
       " ('stained', 1),\n",
       " ('blood', 1),\n",
       " ('strange', 1),\n",
       " ('plants', 1),\n",
       " ('easily', 1),\n",
       " ('fallen', 1),\n",
       " ('asleep', 1),\n",
       " ('staffroom', 1),\n",
       " ('morning', 1),\n",
       " ('teach', 1),\n",
       " ('leaving', 1),\n",
       " ('freezing', 1),\n",
       " ('forget', 1),\n",
       " ('hissed', 1),\n",
       " ('witch', 1),\n",
       " ('stirring', 1),\n",
       " ('cauldrons', 1),\n",
       " ('wonderful', 1),\n",
       " ('week', 1),\n",
       " ('exam', 1),\n",
       " ('results', 1),\n",
       " ('quills', 1),\n",
       " ('roll', 1),\n",
       " ('parchment', 1),\n",
       " ('couldn', 1),\n",
       " ('whispered', 1),\n",
       " ('listened', 1),\n",
       " ('rustling', 1),\n",
       " ('clinking', 1),\n",
       " ('coming', 1),\n",
       " ('sounds', 1),\n",
       " ('moving', 1),\n",
       " ('reached', 1),\n",
       " ('later', 1),\n",
       " ('alongside', 1),\n",
       " ('story', 1),\n",
       " ('happening', 1),\n",
       " ('dobby', 1),\n",
       " ('deserted', 1),\n",
       " ('muttering', 1),\n",
       " ('breath', 1),\n",
       " ('gloomy', 1),\n",
       " ('ragged', 1),\n",
       " ('chains', 1),\n",
       " ('cheerful', 1),\n",
       " ('knight', 1),\n",
       " ('sticking', 1),\n",
       " ('ghosts', 1),\n",
       " ('died', 1),\n",
       " ('october', 1),\n",
       " ('amazed', 1),\n",
       " ('approached', 1),\n",
       " ('low', 1),\n",
       " ('mouth', 1),\n",
       " ('taste', 1),\n",
       " ('walk', 1),\n",
       " ('expect', 1),\n",
       " ('stronger', 1),\n",
       " ('flavor', 1),\n",
       " ('didn', 1),\n",
       " ('mind', 1),\n",
       " ('hello', 1),\n",
       " ('squat', 1),\n",
       " ('glided', 1),\n",
       " ('half', 1),\n",
       " ('hidden', 1),\n",
       " ('bitterly', 1),\n",
       " ('dungeon', 1),\n",
       " ('burst', 1),\n",
       " ('dozen', 1),\n",
       " ('wildly', 1),\n",
       " ('clap', 1),\n",
       " ('middle', 1),\n",
       " ('dance', 1),\n",
       " ('halted', 1),\n",
       " ('plunging', 1),\n",
       " ('air', 1),\n",
       " ('crowd', 1),\n",
       " ('strode', 1),\n",
       " ('schedule', 1),\n",
       " ('exciting', 1),\n",
       " ('classes', 1),\n",
       " ('entering', 1),\n",
       " ('blackboard', 1),\n",
       " ('ancient', 1),\n",
       " ('shriveled', 1),\n",
       " ('lungs', 1),\n",
       " ('stop', 1),\n",
       " ('screamed', 1),\n",
       " ('mortal', 1),\n",
       " ('run', 1),\n",
       " ('lives', 1),\n",
       " ('real', 1),\n",
       " ('panic', 1),\n",
       " ('curiously', 1),\n",
       " ('fate', 1),\n",
       " ('worry', 1),\n",
       " ('possibly', 1),\n",
       " ('terrible', 1),\n",
       " ('power', 1),\n",
       " ('harm', 1),\n",
       " ('book', 1),\n",
       " ('awkwardly', 1),\n",
       " ('portrait', 1),\n",
       " ('hole', 1),\n",
       " ('immediately', 1),\n",
       " ('friend', 1),\n",
       " ('lee', 1),\n",
       " ('jordan', 1),\n",
       " ('fingers', 1),\n",
       " ('stand', 1),\n",
       " ('lockhart', 1),\n",
       " ('getting', 1),\n",
       " ('feet', 1),\n",
       " ('ways', 1),\n",
       " ('aside', 1),\n",
       " ('pipe', 1),\n",
       " ('miles', 1),\n",
       " ('weird', 1),\n",
       " ('misty', 1),\n",
       " ('shining', 1),\n",
       " ('day', 1),\n",
       " ('older', 1),\n",
       " ('sixteen', 1),\n",
       " ('uncertainly', 1),\n",
       " ('fifty', 1),\n",
       " ('popped', 1),\n",
       " ('walls', 1),\n",
       " ('tables', 1),\n",
       " ('great', 1),\n",
       " ('success', 1),\n",
       " ('pleasant', 1),\n",
       " ('evening', 1),\n",
       " ('mood', 1),\n",
       " ('lupin', 1),\n",
       " ('forgive', 1),\n",
       " ('believing', 1),\n",
       " ('spy', 1),\n",
       " ('grin', 1),\n",
       " ('flitted', 1),\n",
       " ('shall', 1),\n",
       " ('kill', 1),\n",
       " ('dad', 1),\n",
       " ('maybe', 1),\n",
       " ('met', 1),\n",
       " ('mere', 1),\n",
       " ('green', 1),\n",
       " ('spell', 1),\n",
       " ('smoke', 1),\n",
       " ('hufflepuffs', 1),\n",
       " ('far', 1),\n",
       " ('hall', 1),\n",
       " ('pearly', 1),\n",
       " ('dressed', 1),\n",
       " ('tonight', 1),\n",
       " ('usual', 1),\n",
       " ('question', 1),\n",
       " ('utterly', 1),\n",
       " ('food', 1),\n",
       " ('throwing', 1),\n",
       " ('silent', 1),\n",
       " ('person', 1),\n",
       " ('control', 1),\n",
       " ('store', 1),\n",
       " ('amused', 1),\n",
       " ('month', 1),\n",
       " ('ideas', 1),\n",
       " ('writing', 1),\n",
       " ('weekly', 1),\n",
       " ('goblin', 1),\n",
       " ('century', 1),\n",
       " ('amazing', 1),\n",
       " ('seriously', 1),\n",
       " ('goblet', 1),\n",
       " ('reckon', 1),\n",
       " ('trying', 1),\n",
       " ('weeks', 1),\n",
       " ('meeting', 1),\n",
       " ('noise', 1),\n",
       " ('loud', 1),\n",
       " ('wailing', 1),\n",
       " ('nearest', 1),\n",
       " ('party', 1),\n",
       " ('playing', 1),\n",
       " ('musical', 1),\n",
       " ('shut', 1),\n",
       " ('miracle', 1),\n",
       " ('sort', 1),\n",
       " ('extra', 1),\n",
       " ('concentrated', 1),\n",
       " ('supposed', 1),\n",
       " ('starting', 1),\n",
       " ('dancing', 1),\n",
       " ('champions', 1),\n",
       " ('suppose', 1),\n",
       " ('gloomily', 1),\n",
       " ('girls', 1),\n",
       " ('second', 1),\n",
       " ('putting', 1),\n",
       " ('swallowed', 1),\n",
       " ('considerable', 1),\n",
       " ('bubbles', 1),\n",
       " ('cross', 1),\n",
       " ('legged', 1),\n",
       " ('taps', 1),\n",
       " ('usually', 1),\n",
       " ('friends', 1),\n",
       " ('prepared', 1),\n",
       " ('anybody', 1),\n",
       " ('path', 1),\n",
       " ('leads', 1),\n",
       " ('goal', 1),\n",
       " ('red', 1),\n",
       " ('widened', 1),\n",
       " ('dense', 1),\n",
       " ('wormtail', 1),\n",
       " ('shouts', 1),\n",
       " ('larger', 1),\n",
       " ('kept', 1),\n",
       " ('squeezing', 1),\n",
       " ('narrow', 1),\n",
       " ('dream', 1),\n",
       " ('pushing', 1),\n",
       " ('himself', 1),\n",
       " ('fell', 1),\n",
       " ('surveyed', 1),\n",
       " ('web', 1),\n",
       " ('connected', 1),\n",
       " ('appearance', 1),\n",
       " ('guessing', 1),\n",
       " ('limped', 1),\n",
       " ('rustle', 1),\n",
       " ('small', 1),\n",
       " ('time', 1),\n",
       " ('snarled', 1),\n",
       " ('landed', 1),\n",
       " ('lightly', 1),\n",
       " ('cage', 1),\n",
       " ('work', 1),\n",
       " ('halfway', 1),\n",
       " ('house', 1),\n",
       " ('parvati', 1),\n",
       " ('patil', 1),\n",
       " ('lavender', 1),\n",
       " ('brown', 1),\n",
       " ('gave', 1),\n",
       " ('friendly', 1),\n",
       " ('leaning', 1),\n",
       " ('winced', 1),\n",
       " ('uncomfortable', 1),\n",
       " ('lean', 1),\n",
       " ('honor', 1),\n",
       " ('bound', 1),\n",
       " ('saying', 1),\n",
       " ('sorting', 1),\n",
       " ('glad', 1),\n",
       " ('reason', 1),\n",
       " ('common', 1),\n",
       " ('kind', 1),\n",
       " ('wheezy', 1),\n",
       " ('cause', 1),\n",
       " ('severe', 1),\n",
       " ('minutes', 1),\n",
       " ('warm', 1),\n",
       " ('drifting', 1),\n",
       " ('stuck', 1),\n",
       " ('revealing', 1),\n",
       " ('dangerously', 1),\n",
       " ('continued', 1),\n",
       " ('hesitated', 1),\n",
       " ('wizards', 1),\n",
       " ('year', 1),\n",
       " ('pansy', 1),\n",
       " ('indignantly', 1),\n",
       " ('smirk', 1),\n",
       " ('moved', 1),\n",
       " ('bigger', 1),\n",
       " ('better', 1),\n",
       " ('luggage', 1),\n",
       " ('rack', 1),\n",
       " ('conscious', 1),\n",
       " ('ginny', 1),\n",
       " ('dean', 1),\n",
       " ('listening', 1),\n",
       " ('bench', 1),\n",
       " ('darkly', 1),\n",
       " ('silvery', 1),\n",
       " ('mass', 1),\n",
       " ('rose', 1),\n",
       " ('revolving', 1),\n",
       " ('slowly', 1),\n",
       " ('pensieve', 1),\n",
       " ('completely', 1),\n",
       " ('curious', 1),\n",
       " ('circumstances', 1),\n",
       " ('brought', 1),\n",
       " ('yeh', 1),\n",
       " ('o', 1),\n",
       " ('governors', 1),\n",
       " ('hagrid', 1),\n",
       " ('stopped', 1),\n",
       " ('woman', 1),\n",
       " ('serenely', 1),\n",
       " ('resumed', 1),\n",
       " ('hoarse', 1),\n",
       " ('whisper', 1),\n",
       " ('corner', 1),\n",
       " ('apparently', 1),\n",
       " ('impression', 1),\n",
       " ('draco', 1),\n",
       " ('inside', 1),\n",
       " ('hour', 1),\n",
       " ('wondering', 1),\n",
       " ('paper', 1),\n",
       " ('snape', 1),\n",
       " ('bored', 1),\n",
       " ('fixed', 1),\n",
       " ('ask', 1),\n",
       " ('hastily', 1),\n",
       " ('recall', 1),\n",
       " ('night', 1),\n",
       " ('dark', 1),\n",
       " ('spells', 1),\n",
       " ('merely', 1),\n",
       " ('bidding', 1),\n",
       " ('trust', 1),\n",
       " ('aware', 1),\n",
       " ('departed', 1),\n",
       " ('soul', 1),\n",
       " ('left', 1),\n",
       " ('earth', 1),\n",
       " ('test', 1),\n",
       " ('boys', 1),\n",
       " ('bathroom', 1),\n",
       " ('risen', 1),\n",
       " ('toilet', 1),\n",
       " ('midair', 1),\n",
       " ('round', 1),\n",
       " ('glasses', 1),\n",
       " ('remembering', 1),\n",
       " ('words', 1),\n",
       " ('before:', 1),\n",
       " ('want', 1),\n",
       " ('tom', 1),\n",
       " ('death', 1),\n",
       " ('apparent', 1),\n",
       " ('expression', 1),\n",
       " ('explain', 1),\n",
       " ('sent', 1),\n",
       " ('knew', 1),\n",
       " ('precious', 1),\n",
       " ('attacked', 1),\n",
       " ('true', 1),\n",
       " ('destroyed', 1),\n",
       " ('thought', 1),\n",
       " ('feel', 1),\n",
       " ('surely', 1),\n",
       " ('horcruxes', 1),\n",
       " ('paced', 1),\n",
       " ('luna', 1),\n",
       " ('checking', 1),\n",
       " ('marauder', 1),\n",
       " ('map', 1),\n",
       " ('permitted', 1),\n",
       " ('twice', 1),\n",
       " ('pausing', 1),\n",
       " ('allow', 1),\n",
       " ('pass', 1),\n",
       " ('drawing', 1),\n",
       " ('attention', 1),\n",
       " ('expected', 1),\n",
       " ('encounter', 1),\n",
       " ('worst', 1),\n",
       " ('forced', 1),\n",
       " ('finally', 1),\n",
       " ('reaching', 1),\n",
       " ('stairs', 1),\n",
       " ('waiting', 1),\n",
       " ('dear', 1),\n",
       " ('boy', 1),\n",
       " ('grasp', 1),\n",
       " ('thrust', 1),\n",
       " ('icy', 1),\n",
       " ('offended', 1),\n",
       " ('transparent', 1),\n",
       " ('pointing', 1),\n",
       " ('finger', 1),\n",
       " ('caught', 1),\n",
       " ('sight', 1),\n",
       " ('raised', 1),\n",
       " ('eyebrows', 1),\n",
       " ('proud', 1),\n",
       " ('close', 1),\n",
       " ('recognized', 1),\n",
       " ('tone', 1),\n",
       " ('percy', 1),\n",
       " ('brother', 1),\n",
       " ('kneeling', 1),\n",
       " ('stared', 1),\n",
       " ('laugh', 1),\n",
       " ('etched', 1),\n",
       " ('chapter', 1),\n",
       " ('thirty', 1),\n",
       " ('elder', 1),\n",
       " ('world', 1),\n",
       " ('ended', 1),\n",
       " ('battle', 1),\n",
       " ('twig', 1),\n",
       " ('strewn', 1),\n",
       " ('ground', 1),\n",
       " ('marked', 1),\n",
       " ('outer', 1),\n",
       " ('edge', 1),\n",
       " ('forest', 1),\n",
       " ('opened', 1),\n",
       " ('truly', 1),\n",
       " ('flesh', 1),\n",
       " ('resembled', 1),\n",
       " ('closely', 1),\n",
       " ('escaped', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts['ghost'].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow along the paper in designing a neural algorithm that can compress the word vectors while retaining most of their informational content. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>\n",
    "For the resulting vectors to actually be informative, the source corpus should have a size of at least a few billion words; on the contrary, our corpus enumerates merely a million words, so we can't expect our results to be as great.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.0:  Sparsity and Stability\n",
    "\n",
    "Our matrix $\\mathbf{X}$ is very sparse; most of its elements are zero. Find what the ratio of non-zero elements is.\n",
    "\n",
    "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`.\n",
    "\n",
    "We will soon need to perform division and find the logarithm of $\\mathbf{X}$. Neither of the two operations are well-defined for $0$.\n",
    "\n",
    "Change the matrix's datatype to a `torch.float` and add a small constant to it (e.g. $0.1$) to ensure numerical stability while maintaining sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_zero_ratio(sparse_matrix: LongTensor) -> float:\n",
    "    non_zero_el = torch.nonzero(sparse_matrix).size(0) # nonzero returns a tensor with all the indexes of non-zero elements\n",
    "    tot_el = torch.numel(sparse_matrix)\n",
    "    return non_zero_el/tot_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108448515107535\n"
     ]
    }
   ],
   "source": [
    "print(non_zero_ratio(X))\n",
    "X = X.float()\n",
    "X = X + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.1: From co-occurrence counts to probabilities\n",
    "From the paper: \n",
    "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appear in the context of word $i$. \n",
    "\n",
    "Complete the function `to_probabilities`, that accepts a co-occurrence matrix and returns the probability matrix $P$. \n",
    "\n",
    "_Hint_: Remember broadcasting and `torch.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_probabilities(count_matrix: FloatTensor) -> FloatTensor:\n",
    "    divider = torch.sum(count_matrix, dim=1)\n",
    "    divider = divider.view(divider.shape[0], 1)\n",
    "    return count_matrix/divider\n",
    "\n",
    "P = to_probabilities(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2: Probing words\n",
    "\n",
    "From the paper:\n",
    "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
    "\n",
    "Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix $\\mathbf{P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$.\n",
    "\n",
    "Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix $\\mathbf{P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(word_i: str, word_j: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:  \n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j]\n",
    "    return probability_matrix[i,j].item()\n",
    "\n",
    "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:\n",
    "    return query(word_i, word_k, vocab, probability_matrix)/query(word_j, word_k, vocab, probability_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42263377158243687\n",
      "24.61305209040928\n",
      "8.710822994956738\n",
      "0.5588329712750296\n",
      "0.0036196233324942884\n",
      "43.71879996208764\n",
      "5.626578741777974\n",
      "0.017819201079668088\n",
      "0.678307074092271\n",
      "2.0382544087903236\n"
     ]
    }
   ],
   "source": [
    "print(probe('tea', 'wand', 'spell', vocab, P))\n",
    "print(probe('tea', 'wand', 'cup', vocab, P))\n",
    "\n",
    "print(probe('voldemort', 'hagrid', 'curse', vocab, P))\n",
    "print(probe('voldemort', 'hagrid', 'beast', vocab, P))\n",
    "\n",
    "print(probe('mcgonagall', 'snape', 'potions', vocab, P))\n",
    "print(probe('mcgonagall', 'snape', 'transfiguration', vocab, P))\n",
    "\n",
    "print(probe('hedwig', 'scabbers', 'owl', vocab, P))\n",
    "print(probe('hedwig', 'scabbers', 'rat', vocab, P))\n",
    "\n",
    "print(probe('ron', 'hermione', 'book', vocab, P))\n",
    "print(probe('ron', 'hermione', 'red', vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it does seem like we are getting sensible results for in-domain words. Of course, probing out-of-domain words (such as the thermodynamics example the authors present) does not go all that well.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08204095852267408\n",
      "0.9024505003445088\n"
     ]
    }
   ],
   "source": [
    "print(probe('ice', 'steam', 'solid', vocab, P))\n",
    "print(probe('ice', 'steam', 'gas', vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Take home message: HP books are probably not the best textbook on thermodynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Vectors\n",
    "\n",
    "Now, we would like to convert these long, spare vectors into short, dense ones. \n",
    "\n",
    "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
    "\n",
    "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
    "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
    "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
    "3. > It should be well-defined for all values in $X$.\n",
    "\n",
    "Given these three constraints, each word $i$ of our vocabulary is represented by four vectors:\n",
    "1. A vector $w_i \\in \\mathbb{R}^D$\n",
    "2. A bias $b_i \\in \\mathbb{R}$\n",
    "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
    "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
    "\n",
    "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
    "\n",
    "$F(w_i, \\tilde{w}_j, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$\n",
    "\n",
    "such that $F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k)$ approximates $log(\\mathbf{X}_{ik})$, \n",
    "\n",
    "or equivallently the least squares error $J$ is minimized, where:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$ the loss term\n",
    "\n",
    "and \n",
    "\n",
    "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
    "    (x/x_{max})^a, & \\text{if $x<x_{max}$}\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}$ a term weighting function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.3: Weighting Function\n",
    "\n",
    "Let's start with the last part. Complete the weighting function `weight_fn` which accepts a co-occurrence matrix $\\mathbf{X}$, a maximum value $x_{max}$ and a fractional power $a$, and returns the weighted co-occurrence matrix $f(\\mathbf{X})$.\n",
    "\n",
    "Then, compute `X_weighted`, the weighting of $\\mathbf{X}$, using the paper's suggested parameters.\n",
    "\n",
    "_Hint_: Note that $f$ is defined point-wise, so our weighting function should also be point-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_fn(X: FloatTensor, x_max: int, a: float) -> FloatTensor:\n",
    "    return torch.where(X<x_max, (X/x_max)**a, torch.ones(X.shape))\n",
    "\n",
    "X_weighted = weight_fn(X, 100, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.4: Loss Function\n",
    "\n",
    "Next step is to write the loss function. \n",
    "\n",
    "We can write it as a point-wise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient. \n",
    "\n",
    "Inspecting the formulation of $J$, it is fairly straight-forward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})\\cdot(W\\tilde{W}^T + b + \\tilde{b}^T - log(X))^2$,\n",
    "\n",
    "where $W$, $\\tilde{W}$ the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
    "\n",
    "Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f(\\mathbf{X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix $\\mathbf{X}$, and computes $J$. What is the expected shape of $J$ before and after the summation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(X_weighted: FloatTensor, W: FloatTensor, W_context: FloatTensor, \n",
    "            B: FloatTensor, B_context: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
    "    loss = torch.sum(X_weighted * (W @ W_context.t() + B + B_context.t() - torch.log(X)) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.5: GloVe\n",
    "\n",
    "We have the normalized co-occurrence matrix $\\mathbf{X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
    "\n",
    "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors. \n",
    "\n",
    "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/nn.html#embedding). Each such layer may be viewed as a stand-alone network, that can be optimized using the standard procedure we have already seen. \n",
    "\n",
    "We will utilize the `nn.Module` class to contain all our embedding layers and steamline their joint optimization.\n",
    "The container class will be responsible for a few things:\n",
    "\n",
    "1. Wrapping the embedding layers:\n",
    "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
    "    1. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
    "    1. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
    "    1. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
    "1. Implementing `forward`, a function that accepts a weighted co-occurrence matrix $f(\\mathbf{X})$, the co-occurrence matrix $\\mathbf{X}$, then computes the embeddings of all words and finally calls `loss_fn` as defined above.\n",
    "1. Implementing `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
    "> .. With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
    "\n",
    "Complete the network class following the above specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(torch.nn.Module):\n",
    "    def __init__(self, vocab: Dict[str, int], vector_dimensionality: int=30, device: str='cpu') -> None:\n",
    "        super(GloVe, self).__init__()\n",
    "        self.device = device\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.w = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=vector_dimensionality).to(self.device)\n",
    "        self.wc = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=vector_dimensionality).to(self.device)\n",
    "        self.b = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=1).to(self.device)\n",
    "        self.bc = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=1).to(self.device)\n",
    "        \n",
    "    def forward(self, X_weighted: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        WC = self.wc(embedding_input)\n",
    "        B = self.b(embedding_input)\n",
    "        BC = self.bc(embedding_input)       \n",
    "        return loss_fn(X_weighted, W, WC, B, BC, X)\n",
    "    \n",
    "    def get_vectors(self) -> FloatTensor:\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        WC = self.wc(embedding_input)\n",
    "        return W + WC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6: Training\n",
    "\n",
    "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign). Instantiate the network class you just defined, using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs.\n",
    "\n",
    "When writing the training script, remember that your network's forward pass is __already__ computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = GloVe(vocab)\n",
    "opt = torch.optim.Adam(network.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 11838599.0\n",
      "Iteration 10 loss: 3171371.5\n",
      "Iteration 20 loss: 1178918.75\n",
      "Iteration 30 loss: 565795.375\n",
      "Iteration 40 loss: 411583.03125\n",
      "Iteration 50 loss: 356158.3125\n",
      "Iteration 60 loss: 332442.75\n",
      "Iteration 70 loss: 320096.34375\n",
      "Iteration 80 loss: 311698.875\n",
      "Iteration 90 loss: 304708.25\n",
      "Iteration 100 loss: 298670.8125\n",
      "Iteration 110 loss: 293599.875\n",
      "Iteration 120 loss: 289392.5\n",
      "Iteration 130 loss: 285875.1875\n",
      "Iteration 140 loss: 282890.8125\n",
      "Iteration 150 loss: 280318.375\n",
      "Iteration 160 loss: 278071.8125\n",
      "Iteration 170 loss: 276092.4375\n",
      "Iteration 180 loss: 274337.75\n",
      "Iteration 190 loss: 272774.9375\n",
      "Iteration 200 loss: 271379.125\n",
      "Iteration 210 loss: 270131.625\n",
      "Iteration 220 loss: 269015.3125\n",
      "Iteration 230 loss: 268013.875\n",
      "Iteration 240 loss: 267113.09375\n",
      "Iteration 250 loss: 266299.4375\n",
      "Iteration 260 loss: 265560.5625\n",
      "Iteration 270 loss: 264884.5\n",
      "Iteration 280 loss: 264261.6875\n",
      "Iteration 290 loss: 263683.875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss = network.forward(X_weighted, X)\n",
    "    loss.backward()\n",
    "    opt.step() \n",
    "    opt.zero_grad() \n",
    "    if i%10 == 0:\n",
    "        print('Iteration {} loss: {}'.format(i, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.7: Validation (Similarity)\n",
    "\n",
    "Curious to see what this network has learned? Let's perform a simple validation experiment. \n",
    "\n",
    "We will check which words the models considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high dimensional vector spaces is the cosine similarity. \n",
    "\n",
    "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
    "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}|_2 \\cdot |\\vec{b}|_2}$$\n",
    "\n",
    "where $|\\vec{x}|$_2 the $L_2$-norm of the $\\vec{x}$.\n",
    "\n",
    "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: FloatTensor) -> float:\n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j] \n",
    "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
    "    v_j = vectors[j] / torch.norm(vectors[j], p=2)  # b/|b|\n",
    "    sim = torch.mm(v_i.view(1, -1), v_j.view(-1, 1)).item()\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples (try your own word pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between cruciatus and imperius is: 0.5489968657493591\n",
      "\n",
      "Similarity between avada and kedavra is: 0.8792098760604858\n",
      "\n",
      "Similarity between hogwarts and school is: 0.5746870040893555\n",
      "\n",
      "Similarity between goblin and hagrid is: 0.19663673639297485\n",
      "\n",
      "Similarity between giant and hagrid is: 0.5904597043991089\n",
      "\n",
      "Similarity between avada and hermione is: -0.12604019045829773\n",
      "\n",
      "Similarity between avada and voldemort is: 0.5594943165779114\n",
      "\n",
      "Similarity between hand and wand is: 0.5754926204681396\n",
      "\n",
      "Similarity between quidditch and dumbledore is: 0.16698962450027466\n",
      "\n",
      "Similarity between professor and snape is: 0.5495306849479675\n",
      "\n",
      "Similarity between professor and petunia is: 0.10922577977180481\n",
      "\n",
      "Similarity between professor and cup is: -0.09921182692050934\n"
     ]
    }
   ],
   "source": [
    "word_vectors = network.get_vectors().detach()\n",
    "\n",
    "for pair in [('cruciatus', 'imperius'), \n",
    "             ('avada', 'kedavra'), \n",
    "             ('hogwarts', 'school'), \n",
    "             ('goblin', 'hagrid'), \n",
    "             ('giant', 'hagrid'),\n",
    "             ('avada', 'hermione'), \n",
    "             ('avada', 'voldemort'), \n",
    "             ('hand', 'wand'),\n",
    "             ('quidditch', 'dumbledore'),\n",
    "             ('professor', 'snape'),\n",
    "             ('professor', 'petunia'),\n",
    "             ('professor', 'cup'),\n",
    "            ]:\n",
    "    \n",
    "    print('\\nSimilarity between {} and {} is: {}'.\n",
    "          format(pair[0], pair[1], similarity(pair[0], pair[1], vocab, word_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the similarities of one word against all other words in the corpus, we may rewrite the above equation as:\n",
    "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}|_2 \\cdot |\\mathbf{C}|_2}$$\n",
    "\n",
    "Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus.\n",
    "\n",
    "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(word_i: str, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
    "    i = vocab[word_i]\n",
    "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
    "    v_C = vectors / torch.norm(vectors, p=2)  # b/|b|\n",
    "    sims = (torch.mm(v_C, v_i.view(-1, 1))).t()\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word_i: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
    "    sims = similarities(word_i, vocab, vectors)\n",
    "    _, topi = sims.topk(dim=-1, k=k)\n",
    "    topi = topi.view(-1).cpu().numpy().tolist()\n",
    "    inv = {v: i for i, v in vocab.items()}\n",
    "    return [inv[i] for i in topi if inv[i] != word_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to forbidden: ['cabin', 'forest', 'grounds', 'path', 'visit']\n",
      "Most similar words to myrtle: ['moaning', 'bathroom', 'toilet', 'dobby', 'funny']\n",
      "Most similar words to gryffindor: ['slytherin', 'quaffle', 'angelina', 'ravenclaw', 'lee']\n",
      "Most similar words to wand: ['dumbledore', 'just', 'said', 'looked', 'right']\n",
      "Most similar words to quidditch: ['team', 'wood', 'match', 'seeker', 'snitch']\n",
      "Most similar words to marauder: ['map', 'parchment', 'tapestry', 'tapped', 'padfoot']\n",
      "Most similar words to horcrux: ['locket', 'diary', 'sword', 't', 'soul']\n",
      "Most similar words to phoenix: ['fawkes', 'wand', 'ollivander', 'cornelius', 'feather']\n",
      "Most similar words to triwizard: ['tournament', 'champions', 'diggory', 'task', 'krum']\n",
      "Most similar words to screaming: ['pain', 'blinding', 'screamed', 'upward', 'wand']\n",
      "Most similar words to letter: ['owl', 'hedwig', 'pigwidgeon', 'owls', 'prophet']\n"
     ]
    }
   ],
   "source": [
    "for word in ['forbidden', 'myrtle', 'gryffindor', 'wand', 'quidditch', 'marauder', 'horcrux', 'phoenix', 'triwizard', 'screaming',\n",
    "            'letter'\n",
    "            ]:\n",
    "    print('Most similar words to {}: {}'.format(word, most_similar(word, vocab, word_vectors, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite impressive; we managed to encode a meaningful portion of the corpus statistics in such $30$ numbers in each word! \n",
    "(A compression ratio of 99.4%)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of $\\mathbf{X}$, $x_{max}$, $a$, the number of epochs and the dimensionality of the vector space).\n",
    "</div>\n",
    "\n",
    "Word vectors, however, contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.8: Shortcomings\n",
    "Evidently, GloVe offers a simple and computationally efficient means to construct dense word representations.\n",
    "However, the means of vectorization suffers from a few important shortcomings.\n",
    "Can you imagine what these are? Briefly report on at least two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first shortcoming is that, through the process of constructing short dense word representations, some information is inevitably lost.\n",
    "\n",
    "Additionally, the dense, short vectors representing words do not carry explicit information about the words anymore, as did the co-occurrence matrix.\n",
    "The 30 dimensions of the embedding obtained with this means of vectorization do not represent anything understandable by a human observer. An obsverver could have interpreted each dimension of the long sparse vectors as the co-occurrence of a word with another word.\n",
    "\n",
    "There are also general shortcomings regarding the adopted approach.\n",
    "One of these is the inability at learning the representation of words that are not present in the vocabulary.\n",
    "This leads GolVe to perform poorly in contexts it has not been trained on.\n",
    "Moreover, words with opposite meaning are usually represented by similar vectors.\n",
    "There are also some shortcomings regarding computational efficiency: The co-occurrence matrix takes a lot of memory for storage, and when it is necessary to modify the hyper-parameter related to it, the whole matrix must be reconstructed again, which for high dimensionality is very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.9: Validation (Word Analogies)\n",
    "\n",
    "From the paper:\n",
    "> The word analogy task consists of questions like \"_a_ is to _b_ as is _c_ to ?\" To correctly answer this question, we must find the word d such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
    "\n",
    "Write your own function that performs the word analogy task.\n",
    "\n",
    "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word_a: str, word_b: str, word_c: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
    "    a = vocab[word_a]\n",
    "    b = vocab[word_b]\n",
    "    c = vocab[word_c]\n",
    "    dt = vectors[b] - vectors[a] + vectors[c]\n",
    "    \n",
    "    v_dt = dt / torch.norm(dt, p=2)\n",
    "    \n",
    "    v_C = vectors / torch.norm(vectors, p=2)\n",
    "    sims = (torch.mm(v_C, v_dt.view(-1, 1))).t()\n",
    "    \n",
    "    _, topi = sims.topk(dim=-1, k=k)\n",
    "    topi = topi.view(-1).cpu().numpy().tolist()\n",
    "    \n",
    "    inv = {v: i for i, v in vocab.items()}\n",
    "    analogies = [inv[i] for i in topi if inv[i] != word_a and inv[i] != word_b and inv[i] != word_c]  \n",
    "    return analogies[0:6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example triplets to test your analogies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padma is to parvati as fred is to ['t', 'said', 'ron', 'weasley', 'd', 'dean']\n",
      "avada is to kedavra as expecto is to ['patronum', 'stag', 'dementor', 'gregorovitch', 'wand', 'wormtail']\n",
      "dungeon is to slytherin as tower is to ['gryffindor', 'hufflepuff', 'wood', 'ravenclaw', 'alicia', 'quaffle']\n",
      "scabbers is to ron as hedwig is to ['t', 'said', 'pigwidgeon', 'harry', 'breakfast', 'd']\n",
      "ron is to molly as draco is to ['department', 'crouch', 'evans', 'lucius', 'greyback', 'police']\n",
      "durmstrang is to viktor as beauxbatons is to ['ludo', 'crouch', 'krum', 'blonde', 'madame', 'cho']\n",
      "snape is to potions as trelawney is to ['divination', 'practical', 'transfiguration', 'pages', 'essay', 'arts']\n",
      "harry is to seeker as ron is to ['snitch', 'team', 'match', 'flint', 'game', 'warrington']\n",
      "malfoy is to slytherin as potter is to ['sorting', 'quaffle', 'gryffindor', 'hufflepuff', 'ravenclaw', 'related']\n",
      "headless is to nick as moaning is to ['bathroom', 'myrtle', 'borgin', 'goyle', 'peeves', 'willow']\n",
      "spiders is to ron as dementors is to ['t', 'sirius', 'said', 've', 'weasley', 'd']\n",
      "buckbeak is to hippogriff as hedwig is to ['beak', 'letter', 'pigwidgeon', 'owl', 'scarlet', 'cage']\n"
     ]
    }
   ],
   "source": [
    "triplets = [('padma', 'parvati', 'fred'),\n",
    "            ('avada', 'kedavra', 'expecto'),\n",
    "            ('dungeon', 'slytherin', 'tower'),\n",
    "            ('scabbers', 'ron', 'hedwig'),\n",
    "            ('ron', 'molly', 'draco'),\n",
    "            ('durmstrang', 'viktor', 'beauxbatons'),\n",
    "            ('snape', 'potions', 'trelawney'),\n",
    "            ('harry', 'seeker', 'ron'),\n",
    "            ('malfoy', 'slytherin', 'potter'),\n",
    "            ('headless', 'nick', 'moaning'),\n",
    "            ('spiders', 'ron', 'dementors'),\n",
    "            ('buckbeak', 'hippogriff', 'hedwig'),\n",
    "           ]\n",
    "\n",
    "for a, b, c in triplets:\n",
    "    print('{} is to {} as {} is to {}'.format(a, b, c, analogy(a, b, c, vocab, word_vectors, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minimal emergent intelligence :) *(hopefully..)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧙‍♀️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional[Extra]\n",
    "How does training and hyper-parameter choice affect the model's performance?\n",
    "Repeat the training using your own hyper-parameters (vector space dimensionality, optimizer parameters, training epochs, etc.). \n",
    "\n",
    "During the training loop, print the qualitative benchmarks every few epochs. Do they keep improving? Is there any disadvantage to exhaustively training until convergence?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Incrementing the vector space dimensionality slightly improves the performance (e.g. with D=100 the loss decreases by ~10%), but is not worth it due to the major computational load (a high compression ratio is preferable when the performance is not drastically different).\n",
    "\n",
    "Lower values of alpha cause the loss to increase significantly (e.g with alpha=0.5 the loss increases by ~200%), while higher values of alpha allow a very low loss to be reached (e.g. with alpha=1.5 the loss decreases by a factor 1/100, with alpha=5 by a factor 1/1000), but make the model lose the ability to represent words similarity, leading to non-sense results in the analogy task.\n",
    "\n",
    "Training the model over a higher number of epochs (e.g. 2000), does not accomplish anything relevant, since the loss decreases only by a few percentual points and after a certain value it remains constant.\n",
    "As a consequence, a higher learning rate is also not useful.\n",
    "\n",
    "Even when exhaustively training until convergence, overfitting would not be a problem in this case, since the model is not expected to be able to generalize to unseen words.\n",
    "Nevertheless, a disadvantage could be the waste of energy due to exhaustive training which is not necessary due to not leading to any significant improvment in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
